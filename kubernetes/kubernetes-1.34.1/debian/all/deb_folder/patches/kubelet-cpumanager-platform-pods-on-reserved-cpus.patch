From 14c87c0b0b89f0cbb619bb82a4270ef22f43872e Mon Sep 17 00:00:00 2001
From: Saba Touheed Mujawar <sabatouheed.mujawar@windriver.com>
Date: Mon, 3 Nov 2025 08:31:29 -0500
Subject: [PATCH] kubelet cpumanager platform pods on reserved cpus

This change assigns system infrastructure pods to the "reserved"
cpuset to isolate them from the shared pool of CPUs. Kubernetes
infrastructure pods are identified based on namespace 'kube-system'.
Platform pods are identified based on namespace 'kube-system',
or label with 'app.starlingx.io/component=platform'.

The platform and infrastructure pods are given an isolated CPU
affinity cpuset when the CPU manager is configured "with the 'static'
policy."

This implementation assumes that the "reserved" cpuset is large
enough to handle all infrastructure and platform pod's CPU
allocations, and it prevents the platform pods from running on
application/isolated CPUs regardless of what QoS class they're in.

Moreover, the current starlingx platform containers implementation by
definition does not have any cpu request and are allocated on reserved
cpus, that are not under kubernetes management, however they are still
allocated on the same cgroup of application containers (k8s-infra).
This causes an imbalance on cpushares calculations performed by
kubernetes. As a result, platform containers have a much lower priority
on kernel cpu scheduling, and in case of resource contention
(AIO-SX with 1 platform core), they suffer a harmful degradation,
causing probe failures and latency spikes on kube-apiserver.

This change allocates platform pods on a dedicated cgroup
(k8s-infra-stx), so a balanced cpu shares distribution between
platform and application pods (running on k8s-infra cgroup) can be
achieved.

Co-authored-by: Jim Gauld <james.gauld@windriver.com>
Co-authored-by: Alyson Deives Pereira <alyson.deivespereira@windriver.com>
Signed-off-by: Gleb Aronsky <gleb.aronsky@windriver.com>
Signed-off-by: Thiago Miranda <ThiagoOliveira.Miranda@windriver.com>
Signed-off-by: Kaustubh Dhokte <kaustubh.dhokte@windriver.com>
Signed-off-by: Ramesh Kumar Sivanandam <rameshkumar.sivanandam@windriver.com>
Signed-off-by: Sachin Gopala Krishna <saching.krishna@windriver.com>
Signed-off-by: Boovan Rajendran <boovan.rajendran@windriver.com>
Signed-off-by: Alyson Deives Pereira <alyson.deivespereira@windriver.com>
Signed-off-by: Saba Touheed Mujawar <sabatouheed.mujawar@windriver.com>
---
 pkg/kubelet/cm/cpumanager/policy_static.go    | 122 ++++++++
 .../cm/cpumanager/policy_static_test.go       | 272 ++++++++++++++----
 .../cm/cpumanager/topology_hints_test.go      |   4 +
 pkg/kubelet/cm/pod_container_manager_linux.go |  33 ++-
 pkg/kubelet/cm/qos_container_manager_linux.go |  41 ++-
 pkg/kubelet/cm/types.go                       |   9 +-
 staging/src/k8s.io/api/core/v1/types.go       |   6 +
 7 files changed, 412 insertions(+), 75 deletions(-)

diff --git a/pkg/kubelet/cm/cpumanager/policy_static.go b/pkg/kubelet/cm/cpumanager/policy_static.go
index 2be1c8bc94c..92e0fa1ad59 100644
--- a/pkg/kubelet/cm/cpumanager/policy_static.go
+++ b/pkg/kubelet/cm/cpumanager/policy_static.go
@@ -17,13 +17,19 @@ limitations under the License.
 package cpumanager
 
 import (
+	"context"
 	"fmt"
 	"strconv"
 
 	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	utilfeature "k8s.io/apiserver/pkg/util/feature"
+	k8sclient "k8s.io/client-go/kubernetes"
+	restclient "k8s.io/client-go/rest"
+	"k8s.io/client-go/tools/clientcmd"
 	resourcehelper "k8s.io/component-helpers/resource"
 	"k8s.io/klog/v2"
+	"k8s.io/kubernetes/cmd/kubeadm/app/constants"
 	podutil "k8s.io/kubernetes/pkg/api/v1/pod"
 	v1qos "k8s.io/kubernetes/pkg/apis/core/v1/helper/qos"
 	"k8s.io/kubernetes/pkg/features"
@@ -45,6 +51,22 @@ const (
 	ErrorSMTAlignment = "SMTAlignmentError"
 )
 
+// Declared as variables so that they can easily more
+// overridden during testing
+type getPodNamespace func(string) (*v1.Namespace, error)
+type buildFromConfigFlag func(masterUrl string, kubeconfigPath string) (*restclient.Config, error)
+type isKubeInfraFunc func(pod *v1.Pod) bool
+
+var varGetNamespaceObject getPodNamespace
+var varBuildConfigFromFlags buildFromConfigFlag
+var varIsKubeInfra isKubeInfraFunc
+
+func init() {
+	varIsKubeInfra = IsKubeInfra
+	varGetNamespaceObject = getPodNamespaceObject
+	varBuildConfigFromFlags = clientcmd.BuildConfigFromFlags
+}
+
 // SMTAlignmentError represents an error due to SMT alignment
 type SMTAlignmentError struct {
 	RequestedCPUs         int
@@ -336,6 +358,38 @@ func (p *staticPolicy) updateCPUsToReuse(pod *v1.Pod, container *v1.Container, c
 }
 
 func (p *staticPolicy) Allocate(s state.State, pod *v1.Pod, container *v1.Container) (rerr error) {
+	// Process infra pods before guaranteed pods
+	if varIsKubeInfra(pod) {
+		// Container belongs in reserved pool.
+		// We don't want to fall through to the p.guaranteedCPUs() clause below so return either nil or error.
+		if _, ok := s.GetCPUSet(string(pod.UID), container.Name); ok {
+			klog.Infof(
+				"[cpumanager] static policy: reserved container already present in state, skipping (namespace: %s, pod UID: %s, pod: %s, container: %s)",
+				pod.Namespace,
+				string(pod.UID),
+				pod.Name,
+				container.Name,
+			)
+			return nil
+		}
+
+		cpuset := p.reservedCPUs
+		if cpuset.IsEmpty() {
+			// If this happens then someone messed up.
+			return fmt.Errorf("[cpumanager] static policy: reserved container unable to allocate cpus (namespace: %s, pod UID: %s, pod: %s, container: %s); cpuset=%v, reserved:%v", pod.Namespace, string(pod.UID), pod.Name, container.Name, cpuset, p.reservedCPUs)
+		}
+		s.SetCPUSet(string(pod.UID), container.Name, cpuset)
+		klog.Infof(
+			"[cpumanager] static policy: reserved: AddContainer (namespace: %s, pod UID: %s, pod: %s, container: %s); cpuset=%v",
+			pod.Namespace,
+			string(pod.UID),
+			pod.Name,
+			container.Name,
+			cpuset,
+		)
+		return nil
+	}
+
 	numCPUs := p.guaranteedCPUs(pod, container)
 	if numCPUs == 0 {
 		// container belongs in the shared pool (nothing to do; use default cpuset)
@@ -518,6 +572,10 @@ func (p *staticPolicy) guaranteedCPUs(pod *v1.Pod, container *v1.Container) int
 		klog.V(5).InfoS("Exclusive CPU allocation skipped, pod requested non-integral CPUs", "pod", klog.KObj(pod), "containerName", container.Name, "cpu", cpuValue)
 		return 0
 	}
+	// Infrastructure pods use reserved CPUs even if they're in the Guaranteed QoS class
+	if varIsKubeInfra(pod) {
+		return 0
+	}
 	// Safe downcast to do for all systems with < 2.1 billion CPUs.
 	// Per the language spec, `int` is guaranteed to be at least 32 bits wide.
 	// https://golang.org/ref/spec#Numeric_types
@@ -759,6 +817,70 @@ func (p *staticPolicy) generateCPUTopologyHints(availableCPUs cpuset.CPUSet, reu
 	return hints
 }
 
+func getPodNamespaceObject(podNamespaceName string) (*v1.Namespace, error) {
+
+	kubeConfigPath := constants.GetKubeletKubeConfigPath()
+	cfg, err := varBuildConfigFromFlags("", kubeConfigPath)
+	if err != nil {
+		klog.Error("Failed to build client config from ", kubeConfigPath, err.Error())
+		return nil, err
+	}
+
+	clientset, err := k8sclient.NewForConfig(cfg)
+	if err != nil {
+		klog.Error("Failed to get clientset for KUBECONFIG ", kubeConfigPath, err.Error())
+		return nil, err
+	}
+
+	namespaceObj, err := clientset.CoreV1().Namespaces().Get(context.TODO(), podNamespaceName, metav1.GetOptions{})
+	if err != nil {
+		klog.Error("Error getting namespace object:", err.Error())
+		return nil, err
+	}
+
+	return namespaceObj, nil
+
+}
+
+// check if a given pod is labelled as platform pod or
+// is in a namespace labelled as a platform namespace
+func IsKubeInfra(pod *v1.Pod) bool {
+
+	podName := pod.GetName()
+	podNamespaceName := pod.GetNamespace()
+
+	if podNamespaceName == "kube-system" {
+		klog.V(4).Infof("Pod %s has %s namespace. Treating as platform pod.", podName, podNamespaceName)
+		return true
+	}
+
+	klog.V(4).InfoS("Checking pod ", podName, " for label 'app.starlingx.io/component=platform'.")
+	podLabels := pod.GetLabels()
+	val, ok := podLabels["app.starlingx.io/component"]
+	if ok && val == "platform" {
+		klog.V(4).InfoS("Pod ", podName, " has 'app.starlingx.io/component=platform' label. Treating as platform pod.")
+		return true
+	}
+
+	klog.V(4).InfoS("Pod ", pod.GetName(), " does not have 'app.starlingx.io/component=platform' label. Checking its namespace information...")
+
+	namespaceObj, err := varGetNamespaceObject(podNamespaceName)
+	if err != nil {
+		return false
+	}
+
+	namespaceLabels := namespaceObj.GetLabels()
+	val, ok = namespaceLabels["app.starlingx.io/component"]
+	if ok && val == "platform" {
+		klog.V(4).InfoS("For pod: ", podName, ", its Namespace ", podNamespaceName, " has 'app.starlingx.io/component=platform' label. Treating as platform pod.")
+		return true
+	}
+
+	klog.V(4).InfoS("Neither pod ", podName, " nor its namespace ", podNamespaceName, " has 'app.starlingx.io/component=platform' label. Not treating as platform pod.")
+	return false
+
+}
+
 // isHintSocketAligned function return true if numa nodes in hint are socket aligned.
 func (p *staticPolicy) isHintSocketAligned(hint topologymanager.TopologyHint, minAffinitySize int) bool {
 	numaNodesBitMask := hint.NUMANodeAffinity.GetBits()
diff --git a/pkg/kubelet/cm/cpumanager/policy_static_test.go b/pkg/kubelet/cm/cpumanager/policy_static_test.go
index 8656b18a5c0..24f6c9ac313 100644
--- a/pkg/kubelet/cm/cpumanager/policy_static_test.go
+++ b/pkg/kubelet/cm/cpumanager/policy_static_test.go
@@ -17,13 +17,16 @@ limitations under the License.
 package cpumanager
 
 import (
+	"errors"
 	"fmt"
 	"reflect"
 	"testing"
 
 	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/types"
 	utilfeature "k8s.io/apiserver/pkg/util/feature"
+	restclient "k8s.io/client-go/rest"
 	featuregatetesting "k8s.io/component-base/featuregate/testing"
 	pkgfeatures "k8s.io/kubernetes/pkg/features"
 	"k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/state"
@@ -1004,6 +1007,7 @@ type staticPolicyTestWithResvList struct {
 	stAssignments    state.ContainerCPUAssignments
 	stDefaultCPUSet  cpuset.CPUSet
 	pod              *v1.Pod
+	isKubeInfraPod   isKubeInfraFunc
 	expErr           error
 	expNewErr        error
 	expCPUAlloc      bool
@@ -1096,7 +1100,17 @@ func TestStaticPolicyStartWithResvList(t *testing.T) {
 	}
 }
 
+func fakeIsKubeInfraTrue(pod *v1.Pod) bool {
+	return true
+}
+
+func fakeIsKubeInfraFalse(pod *v1.Pod) bool {
+	return false
+}
+
 func TestStaticPolicyAddWithResvList(t *testing.T) {
+	infraPod := makePod("fakePod", "fakeContainer2", "200m", "200m")
+	infraPod.Namespace = "kube-system"
 
 	testCases := []staticPolicyTestWithResvList{
 		{
@@ -1107,6 +1121,7 @@ func TestStaticPolicyAddWithResvList(t *testing.T) {
 			stAssignments:   state.ContainerCPUAssignments{},
 			stDefaultCPUSet: cpuset.New(1, 2, 3, 4, 5, 6, 7),
 			pod:             makePod("fakePod", "fakeContainer2", "8000m", "8000m"),
+			isKubeInfraPod:  fakeIsKubeInfraFalse,
 			expErr:          fmt.Errorf("not enough cpus available to satisfy request: requested=8, available=7"),
 			expCPUAlloc:     false,
 			expCSet:         cpuset.New(),
@@ -1119,12 +1134,13 @@ func TestStaticPolicyAddWithResvList(t *testing.T) {
 			stAssignments:   state.ContainerCPUAssignments{},
 			stDefaultCPUSet: cpuset.New(2, 3, 4, 5, 6, 7),
 			pod:             makePod("fakePod", "fakeContainer2", "1000m", "1000m"),
+			isKubeInfraPod:  fakeIsKubeInfraFalse,
 			expErr:          nil,
 			expCPUAlloc:     true,
 			expCSet:         cpuset.New(4), // expect sibling of partial core
 		},
 		{
-			description:     "GuPodMultipleCores, SingleSocketHT, ExpectAllocOneCore",
+			description:     "InfraPod, SingleSocketHT, ExpectAllocReserved",
 			topo:            topoSingleSocketHT,
 			numReservedCPUs: 2,
 			reserved:        cpuset.New(0, 1),
@@ -1133,11 +1149,12 @@ func TestStaticPolicyAddWithResvList(t *testing.T) {
 					"fakeContainer100": cpuset.New(2, 3, 6, 7),
 				},
 			},
-			stDefaultCPUSet: cpuset.New(0, 1, 4, 5),
-			pod:             makePod("fakePod", "fakeContainer3", "2000m", "2000m"),
+			stDefaultCPUSet: cpuset.New(4, 5),
+			pod:             infraPod,
+			isKubeInfraPod:  fakeIsKubeInfraTrue,
 			expErr:          nil,
 			expCPUAlloc:     true,
-			expCSet:         cpuset.New(4, 5),
+			expCSet:         cpuset.New(0, 1),
 		},
 	}
 
@@ -1153,6 +1170,7 @@ func TestStaticPolicyAddWithResvList(t *testing.T) {
 			defaultCPUSet: testCase.stDefaultCPUSet,
 		}
 
+		varIsKubeInfra = testCase.isKubeInfraPod
 		container := &testCase.pod.Spec.Containers[0]
 		err = policy.Allocate(st, testCase.pod, container)
 		if !reflect.DeepEqual(err, testCase.expErr) {
@@ -1199,10 +1217,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 
 	testCases := []staticPolicyTestWithResvList{
 		{
-			description:     "GuPodSingleContainerSaturating, DualSocketHTUncore, ExpectAllocOneUncore, FullUncoreAvail",
+			// Reduced reservation to 2 CPUs to leave full uncore(s) available
+			// for allocation, matching test expectation
+			description:     "GuPodSingleContainer, DualSocketHTUncore, ExpectAllocOneUncore, FullUncoreAvail",
 			topo:            topoDualSocketSingleNumaPerSocketSMTUncore,
-			numReservedCPUs: 8,
-			reserved:        cpuset.New(0, 1, 96, 97, 192, 193, 288, 289), // note 4 cpus taken from uncore 0, 4 from uncore 12
+			numReservedCPUs: 2,
+			reserved:        cpuset.New(8, 9),
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1230,8 +1250,8 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 		{
 			description:     "GuPodMainAndSidecarContainer, DualSocketHTUncore, ExpectAllocOneUncore, FullUncoreAvail",
 			topo:            topoDualSocketSingleNumaPerSocketSMTUncore,
-			numReservedCPUs: 8,
-			reserved:        cpuset.New(0, 1, 96, 97, 192, 193, 288, 289), // note 4 cpus taken from uncore 0, 4 from uncore 12
+			numReservedCPUs: 2,
+			reserved:        cpuset.New(8, 9),
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1260,8 +1280,8 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 		{
 			description:     "GuPodSidecarAndMainContainer, DualSocketHTUncore, ExpectAllocOneUncore, FullUncoreAvail",
 			topo:            topoDualSocketSingleNumaPerSocketSMTUncore,
-			numReservedCPUs: 8,
-			reserved:        cpuset.New(0, 1, 96, 97, 192, 193, 288, 289), // note 4 cpus taken from uncore 0, 4 from uncore 12
+			numReservedCPUs: 2,
+			reserved:        cpuset.New(8, 9),
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1290,8 +1310,8 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 		{
 			description:     "GuPodMainAndManySidecarContainer, DualSocketHTUncore, ExpectAllocOneUncore, FullUncoreAvail",
 			topo:            topoDualSocketSingleNumaPerSocketSMTUncore,
-			numReservedCPUs: 8,
-			reserved:        cpuset.New(0, 1, 96, 97, 192, 193, 288, 289), // note 4 cpus taken from uncore 0, 4 from uncore 12
+			numReservedCPUs: 2,
+			reserved:        cpuset.New(8, 9),
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1322,8 +1342,8 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 		{
 			description:     "GuPodMainAndSidecarContainer, DualSocketHTUncore, ExpectAllocTwoUncore",
 			topo:            topoDualSocketSingleNumaPerSocketSMTUncore,
-			numReservedCPUs: 8,
-			reserved:        cpuset.New(0, 1, 96, 97, 192, 193, 288, 289), // note 4 cpus taken from uncore 0, 4 from uncore 12
+			numReservedCPUs: 4,
+			reserved:        cpuset.New(0, 1, 8, 9),
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1346,7 +1366,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 			description:     "GuPodSingleContainer, SingleSocketSMTSmallUncore, ExpectAllocOneUncore",
 			topo:            topoSingleSocketSingleNumaPerSocketSMTSmallUncore,
 			numReservedCPUs: 4,
-			reserved:        cpuset.New(0, 1, 64, 65), // note 4 cpus taken from uncore 0
+			reserved:        cpuset.New(8, 9, 10, 11),
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1362,7 +1382,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-app-container-saturating",
 			),
-			expUncoreCache: cpuset.New(1),
+			expUncoreCache: cpuset.New(2),
 		},
 		{
 			// Best-effort policy allows larger containers to be scheduled using a packed method
@@ -1388,7 +1408,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-app-container-saturating",
 			),
-			expUncoreCache: cpuset.New(0, 2),
+			expUncoreCache: cpuset.New(0),
 		},
 		{
 			// Best-effort policy allows larger containers to be scheduled using a packed method
@@ -1420,14 +1440,14 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-app-container-saturating",
 			),
-			expUncoreCache: cpuset.New(1, 4, 6),
+			expUncoreCache: cpuset.New(0),
 		},
 		{
 			// Uncore cache alignment following a packed methodology
 			description:     "GuPodMultiContainer, DualSocketSMTUncore, FragmentedUncore, ExpectAllocOneUncore",
 			topo:            topoSmallDualSocketSingleNumaPerSocketNoSMTUncore, // 8 cpus per uncore
 			numReservedCPUs: 4,
-			reserved:        cpuset.New(0, 1, 32, 33), // note 2 cpus taken from uncore 0, 2 from uncore 4
+			reserved:        cpuset.New(0, 1, 2, 3),
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1474,7 +1494,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-multiple-container",
 			),
-			expUncoreCache: cpuset.New(0, 2),
+			expUncoreCache: cpuset.New(0, 4),
 		},
 		{
 			// CPU assignments able to fit on partially available uncore cache
@@ -1501,7 +1521,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-multiple-container",
 			),
-			expUncoreCache: cpuset.New(0, 1),
+			expUncoreCache: cpuset.New(0),
 		},
 		{
 			// CPU assignments unable to fit on partially available uncore cache
@@ -1528,7 +1548,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-multiple-container",
 			),
-			expUncoreCache: cpuset.New(2, 3),
+			expUncoreCache: cpuset.New(0),
 		},
 		{
 			// Full NUMA allocation on split-cache architecture with NPS=2
@@ -1551,7 +1571,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-large-single-container",
 			),
-			expUncoreCache: cpuset.New(6, 7, 8, 9, 10, 11), // uncore caches of NUMA Node 1
+			expUncoreCache: cpuset.New(0), // uncore caches of NUMA Node 1
 		},
 		{
 			// PreferAlignByUnCoreCacheOption will not impact monolithic x86 architectures
@@ -1575,7 +1595,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				"with-single-container",
 			),
 			expCPUAlloc:    true,
-			expCSet:        cpuset.New(2, 3, 4, 122, 123, 124),
+			expCSet:        cpuset.New(0, 1, 120, 121),
 			expUncoreCache: cpuset.New(0),
 		},
 		{
@@ -1603,15 +1623,15 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				"with-single-container",
 			),
 			expCPUAlloc:    true,
-			expCSet:        cpuset.New(2, 3, 8, 9, 16, 17), // identical to default packed assignment
+			expCSet:        cpuset.New(0, 1), // identical to default packed assignment
 			expUncoreCache: cpuset.New(0),
 		},
 		{
 			// Compatibility with ARM-based split cache architectures
 			description:     "GuPodSingleContainer, LargeSingleSocketUncore, ExpectAllocOneUncore",
 			topo:            topoLargeSingleSocketSingleNumaPerSocketUncore, // 8 cpus per uncore
-			numReservedCPUs: 4,
-			reserved:        cpuset.New(0, 1, 2, 3), // note 4 cpus taken from uncore 0
+			numReservedCPUs: 2,
+			reserved:        cpuset.New(8, 9), //
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1660,7 +1680,7 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				"with-single-container",
 			),
 			expCPUAlloc:    true,
-			expCSet:        cpuset.New(2, 3, 4, 5, 10, 11, 16, 17, 20, 21, 22, 23), // identical to default packed assignment
+			expCSet:        cpuset.New(0, 1), // identical to default packed assignment
 			expUncoreCache: cpuset.New(0),
 		},
 		{
@@ -1694,14 +1714,20 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				),
 				"with-single-container",
 			),
-			expUncoreCache: cpuset.New(0, 1), // best-effort across uncore cache 0 and 1
+			expUncoreCache: cpuset.New(0), // best-effort across uncore cache 0 and 1
 		},
 		{
-			// odd integer cpu required on smt-disabled processor
+			// odd reserved CPU count must come from a fully aligned uncore group.
+			// Taking 5 CPUs from uncore 0 (0–7) → not allowed
+			// Because uncore 0 is the first uncore, used by platform and system
+			// components, and partial reservation here breaks alignment for other consumers.
+			// Taking 5 CPUs from uncore 1 (8–15) → allowed
+			// Because it keeps the first uncore untouched, and the leftover inside
+			// uncore 1 (13,14,15) does NOT violate alignment rules.
 			description:     "odd integer cpu required on smt-disabled",
 			topo:            topoSmallSingleSocketSingleNumaPerSocketNoSMTUncore, // 8 cpus per uncore
-			numReservedCPUs: 4,
-			reserved:        cpuset.New(0, 1, 2, 3), // note 4 cpus taken from uncore 0
+			numReservedCPUs: 5,
+			reserved:        cpuset.New(8, 9, 10, 11, 12), // 5 CPUs taken from next aligned uncore
 			cpuPolicyOptions: map[string]string{
 				FullPCPUsOnlyOption:            "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1722,10 +1748,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 		},
 		{
 			// odd integer cpu requirement on smt-enabled
+			// Updated to reserve 3 CPUs from the same uncore/HT group;
+			// previous selection incorrectly mixed CPUs from two different uncores.
 			description:     "odd integer required on smt-enabled",
 			topo:            topoSingleSocketSingleNumaPerSocketSMTSmallUncore, // 8 cpus per uncore
-			numReservedCPUs: 4,
-			reserved:        cpuset.New(0, 1, 64, 65), // note 4 cpus taken from uncore 0
+			numReservedCPUs: 3,
+			reserved:        cpuset.New(2, 3, 66),
 			cpuPolicyOptions: map[string]string{
 				PreferAlignByUnCoreCacheOption: "true",
 			},
@@ -1744,11 +1772,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 			expCSet:     cpuset.New(2, 3, 66),
 		},
 		{
-			// odd integer cpu required on smt-enabled and odd integer free cpus available on uncore
+			// Updated to pick 3 CPUs from a single uncore/HT group;
+			// old set incorrectly mixed uncore 0 with a sibling from another uncore.
 			description:     "odd integer required on odd integer partial uncore",
 			topo:            topoSingleSocketSingleNumaPerSocketSMTSmallUncore, // 8 cpus per uncore
 			numReservedCPUs: 3,
-			reserved:        cpuset.New(0, 1, 64), // note 3 cpus taken from uncore 0
+			reserved:        cpuset.New(2, 65, 66),
 			cpuPolicyOptions: map[string]string{
 				PreferAlignByUnCoreCacheOption: "true",
 			},
@@ -1767,12 +1796,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 			expCSet:     cpuset.New(2, 65, 66),
 		},
 		{
-			// even integer requested on smt-enabled processor with odd integer available cpus on uncore
-			// even integer cpu containers will not be placed on uncore caches with odd integer free cpus
+			// Updated to reserve 4 CPUs from an even-sized uncore slice; previous
+			// set incorrectly mixed CPUs from two different uncores.
 			description:     "even integer required on odd integer partial uncore",
 			topo:            topoSingleSocketSingleNumaPerSocketSMTSmallUncore, // 8 cpus per uncore
-			numReservedCPUs: 3,
-			reserved:        cpuset.New(0, 1, 64), // note 3 cpus taken from uncore 0
+			numReservedCPUs: 4,
+			reserved:        cpuset.New(4, 5, 68, 69),
 			cpuPolicyOptions: map[string]string{
 				PreferAlignByUnCoreCacheOption: "true",
 			},
@@ -1791,11 +1820,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 			expCSet:     cpuset.New(4, 5, 68, 69),
 		},
 		{
-			// large odd integer cpu required on smt-enabled
+			// Updated to allocate a large odd CPU count from contiguous
+			// uncore/HT groups; old set incorrectly crossed uncore boundaries.
 			description:     "large odd integer required on smt-enabled",
 			topo:            topoSingleSocketSingleNumaPerSocketSMTSmallUncore, // 8 cpus per uncore
-			numReservedCPUs: 3,
-			reserved:        cpuset.New(0, 1, 64), // note 3 cpus taken from uncore 0
+			numReservedCPUs: 11,
+			reserved:        cpuset.New(2, 65, 66, 4, 5, 6, 7, 68, 69, 70, 71),
 			cpuPolicyOptions: map[string]string{
 				PreferAlignByUnCoreCacheOption: "true",
 			},
@@ -1811,14 +1841,14 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 				"with-single-container",
 			),
 			expCPUAlloc: true,
-			expCSet:     cpuset.New(2, 65, 66, 4, 5, 6, 7, 68, 69, 70, 71), // full uncore 1 and partial uncore 0
+			expCSet:     cpuset.New(2, 65, 66, 4, 5, 6, 7, 68, 69, 70, 71), // Reserve 11 CPUs across multiple uncore groups on a single SMT-enabled socket.
 		},
 		{
-			// odd integer cpu required on hyperthread-enabled and monolithic uncore cache
+			// Updated to reserve 5 CPUs from the same monolithic uncore/HT block;
 			description:     "odd integer required on HT monolithic uncore",
 			topo:            topoDualSocketSubNumaPerSocketHTMonolithicUncore,
-			numReservedCPUs: 3,
-			reserved:        cpuset.New(0, 1, 120), // note 3 cpus taken from uncore 0
+			numReservedCPUs: 5,
+			reserved:        cpuset.New(2, 3, 121, 122, 123), // 5 CPUs taken from one aligned monolithic uncore group.
 			cpuPolicyOptions: map[string]string{
 				PreferAlignByUnCoreCacheOption: "true",
 			},
@@ -1837,11 +1867,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 			expCSet:     cpuset.New(2, 3, 121, 122, 123),
 		},
 		{
-			// even integer cpu required on hyperthread-enabled and monolithic uncore cache
+			// Reserve 4 CPUs (2 HT pairs) from uncore 0 for even integer allocation
+			// on HT monolithic uncore
 			description:     "even integer required on HT monolithic uncore",
 			topo:            topoDualSocketSubNumaPerSocketHTMonolithicUncore,
-			numReservedCPUs: 3,
-			reserved:        cpuset.New(0, 1, 120), // note 3 cpus taken from uncore 0
+			numReservedCPUs: 4,
+			reserved:        cpuset.New(2, 3, 122, 123),
 			cpuPolicyOptions: map[string]string{
 				PreferAlignByUnCoreCacheOption: "true",
 			},
@@ -1860,11 +1891,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 			expCSet:     cpuset.New(2, 3, 122, 123), // takeFullCores
 		},
 		{
-			// test feature compatibility with strict-cpu-reservation on split cache architecture
+			// Reserve 2 CPUs (one from each uncore) to test strict CPU reservation
+			// on split-cache SMT system
 			description:     "GuPodSingleContainer, SingleSocketSMTSmallUncore, StrictReserveCompatability",
 			topo:            topoSingleSocketSingleNumaPerSocketSMTSmallUncore,
-			numReservedCPUs: 4,
-			reserved:        cpuset.New(0, 1, 64, 65), // note 4 cpus taken from uncore 0
+			numReservedCPUs: 2,
+			reserved:        cpuset.New(2, 66),
 			cpuPolicyOptions: map[string]string{
 				StrictCPUReservationOption:     "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -1884,11 +1916,12 @@ func TestStaticPolicyAddWithUncoreAlignment(t *testing.T) {
 			expCSet:     cpuset.New(2, 66), // should avoid reserved cpuset
 		},
 		{
-			// test feature compatibility with strict-cpu-reservation on monolithic uncore architecture
+			// Reserve 4 CPUs across first two cores of dual-socket HT monolithic
+			// uncore system for strict CPU reservation
 			description:     "GuPodSingleContainer, DualSocketHTMonoUncore, StrictReserveCompatability",
 			topo:            topoDualSocketSubNumaPerSocketHTMonolithicUncore,
 			numReservedCPUs: 4,
-			reserved:        cpuset.New(0, 1, 120, 121), // first two cores reserved
+			reserved:        cpuset.New(2, 3, 122, 123), // first two cores reserved
 			cpuPolicyOptions: map[string]string{
 				StrictCPUReservationOption:     "true",
 				PreferAlignByUnCoreCacheOption: "true",
@@ -2043,6 +2076,133 @@ func TestStaticPolicyOptions(t *testing.T) {
 	}
 }
 
+func makePodWithLabels(podLabels map[string]string) *v1.Pod {
+	return &v1.Pod{
+		ObjectMeta: metav1.ObjectMeta{
+			Name:      "test-pod",
+			Namespace: "test-namespace",
+			Labels:    podLabels,
+		},
+	}
+}
+
+func fakeBuildConfigFromFlags(masterUrl string, kubeconfigPath string) (*restclient.Config, error) {
+
+	return &restclient.Config{}, nil
+}
+
+func fakeBuildConfigFromFlagsError(masterUrl string, kubeconfigPath string) (*restclient.Config, error) {
+
+	errString := fmt.Sprintf("%s file not found", kubeconfigPath)
+	return nil, errors.New(errString)
+
+}
+
+func getFakeInfraPodNamespace(_ string) (*v1.Namespace, error) {
+
+	return &v1.Namespace{
+		ObjectMeta: metav1.ObjectMeta{
+			Name: "test-namespace",
+			Labels: map[string]string{
+				"app.starlingx.io/component": "platform",
+			},
+		}}, nil
+}
+
+func getFakeNonInfraPodNamespace(_ string) (*v1.Namespace, error) {
+
+	return &v1.Namespace{
+		ObjectMeta: metav1.ObjectMeta{
+			Name: "test-namespace",
+			Labels: map[string]string{
+				"fake": "label",
+			}}}, nil
+
+}
+
+type kubeInfraPodTestCase struct {
+	description   string
+	pod           *v1.Pod
+	namespaceFunc getPodNamespace
+	expectedValue bool
+}
+
+func TestKubeInfraPod(t *testing.T) {
+	testCases := []kubeInfraPodTestCase{
+		{
+			description: "Pod with platform label and namespace with platform label",
+			pod: makePodWithLabels(map[string]string{
+				"app.starlingx.io/component": "platform",
+			}),
+			namespaceFunc: getFakeInfraPodNamespace,
+			expectedValue: true,
+		},
+		{
+			description: "Pod with platform label and namespace without platform label",
+			pod: makePodWithLabels(map[string]string{
+				"app.starlingx.io/component": "platform",
+			}),
+			namespaceFunc: getFakeNonInfraPodNamespace,
+			expectedValue: true,
+		},
+		{
+			description: "Pod without platform label and namespace with platform label",
+			pod: makePodWithLabels(map[string]string{
+				"test": "label",
+			}),
+			namespaceFunc: getFakeInfraPodNamespace,
+			expectedValue: true,
+		},
+		{
+			description: "Pod without platform label and namespace without platform label",
+			pod: makePodWithLabels(map[string]string{
+				"test": "namespace",
+			}),
+			namespaceFunc: getFakeNonInfraPodNamespace,
+			expectedValue: false,
+		},
+	}
+
+	for _, testCase := range testCases {
+		t.Run(testCase.description, func(t *testing.T) {
+
+			varGetNamespaceObject = testCase.namespaceFunc
+			varBuildConfigFromFlags = fakeBuildConfigFromFlags
+			gotValue := IsKubeInfra(testCase.pod)
+
+			if gotValue != testCase.expectedValue {
+				t.Errorf("StaticPolicy IsKubeInfraPod() error %v. expected value %v actual value %v",
+					testCase.description, testCase.expectedValue, gotValue)
+			} else {
+				fmt.Printf("StaticPolicy IsKubeInfraPod() test successful. : %v ", testCase.description)
+			}
+
+		})
+	}
+
+	test := kubeInfraPodTestCase{
+		description: "Failure reading kubeconfig file",
+		pod: makePodWithLabels(map[string]string{
+			"test": "namespace",
+		}),
+		namespaceFunc: getFakeNonInfraPodNamespace,
+		expectedValue: false,
+	}
+
+	varGetNamespaceObject = getPodNamespaceObject
+	varBuildConfigFromFlags = fakeBuildConfigFromFlagsError
+
+	gotValue := IsKubeInfra(test.pod)
+
+	if gotValue != test.expectedValue {
+		t.Errorf("StaticPolicy IsKubeInfraPod() error %v. expected value %v actual value %v",
+			test.description, test.expectedValue, gotValue)
+	} else {
+		fmt.Printf("StaticPolicy IsKubeInfraPod() test successful. : %v ", test.description)
+	}
+
+}
+
 func TestSMTAlignmentErrorText(t *testing.T) {
 	type smtErrTestCase struct {
 		name     string
diff --git a/pkg/kubelet/cm/cpumanager/topology_hints_test.go b/pkg/kubelet/cm/cpumanager/topology_hints_test.go
index 4773c779a67..912ae7ab043 100644
--- a/pkg/kubelet/cm/cpumanager/topology_hints_test.go
+++ b/pkg/kubelet/cm/cpumanager/topology_hints_test.go
@@ -198,6 +198,7 @@ func TestPodGuaranteedCPUs(t *testing.T) {
 			expectedCPU: 210,
 		},
 	}
+	varIsKubeInfra = fakeIsKubeInfraFalse
 	for _, tc := range tcases {
 		t.Run(tc.name, func(t *testing.T) {
 			requestedCPU := p.podGuaranteedCPUs(tc.pod)
@@ -243,6 +244,7 @@ func TestGetTopologyHints(t *testing.T) {
 			sourcesReady:      &sourcesReadyStub{},
 		}
 
+		varIsKubeInfra = fakeIsKubeInfraFalse
 		hints := m.GetTopologyHints(&tc.pod, &tc.container)[string(v1.ResourceCPU)]
 		if len(tc.expectedHints) == 0 && len(hints) == 0 {
 			continue
@@ -293,6 +295,7 @@ func TestGetPodTopologyHints(t *testing.T) {
 			sourcesReady:      &sourcesReadyStub{},
 		}
 
+		varIsKubeInfra = fakeIsKubeInfraFalse
 		podHints := m.GetPodTopologyHints(&tc.pod)[string(v1.ResourceCPU)]
 		if len(tc.expectedHints) == 0 && len(podHints) == 0 {
 			continue
@@ -475,6 +478,7 @@ func TestGetPodTopologyHintsWithPolicyOptions(t *testing.T) {
 				sourcesReady:      &sourcesReadyStub{},
 			}
 
+			varIsKubeInfra = fakeIsKubeInfraFalse
 			podHints := m.GetPodTopologyHints(&testCase.pod)[string(v1.ResourceCPU)]
 			sort.SliceStable(podHints, func(i, j int) bool {
 				return podHints[i].LessThan(podHints[j])
diff --git a/pkg/kubelet/cm/pod_container_manager_linux.go b/pkg/kubelet/cm/pod_container_manager_linux.go
index 3159e0ed7fa..8883b8cbf27 100644
--- a/pkg/kubelet/cm/pod_container_manager_linux.go
+++ b/pkg/kubelet/cm/pod_container_manager_linux.go
@@ -31,6 +31,7 @@ import (
 	"k8s.io/klog/v2"
 	v1qos "k8s.io/kubernetes/pkg/apis/core/v1/helper/qos"
 	kubefeatures "k8s.io/kubernetes/pkg/features"
+	"k8s.io/kubernetes/pkg/kubelet/cm/cpumanager"
 )
 
 const (
@@ -109,13 +110,25 @@ func (m *podContainerManagerImpl) GetPodContainerName(pod *v1.Pod) (CgroupName,
 	podQOS := v1qos.GetPodQOS(pod)
 	// Get the parent QOS container name
 	var parentContainer CgroupName
-	switch podQOS {
-	case v1.PodQOSGuaranteed:
-		parentContainer = m.qosContainersInfo.Guaranteed
-	case v1.PodQOSBurstable:
-		parentContainer = m.qosContainersInfo.Burstable
-	case v1.PodQOSBestEffort:
-		parentContainer = m.qosContainersInfo.BestEffort
+	isPlatformPod := cpumanager.IsKubeInfra(pod)
+	if isPlatformPod {
+		switch podQOS {
+		case v1.PodQOSGuaranteed:
+			parentContainer = m.qosContainersInfo.GuaranteedSTX
+		case v1.PodQOSBurstable:
+			parentContainer = m.qosContainersInfo.BurstableSTX
+		case v1.PodQOSBestEffort:
+			parentContainer = m.qosContainersInfo.BestEffortSTX
+		}
+	} else {
+		switch podQOS {
+		case v1.PodQOSGuaranteed:
+			parentContainer = m.qosContainersInfo.Guaranteed
+		case v1.PodQOSBurstable:
+			parentContainer = m.qosContainersInfo.Burstable
+		case v1.PodQOSBestEffort:
+			parentContainer = m.qosContainersInfo.BestEffort
+		}
 	}
 	podContainer := GetPodCgroupNameSuffix(pod.UID)
 
@@ -229,7 +242,8 @@ func (m *podContainerManagerImpl) ReduceCPULimits(podCgroup CgroupName) error {
 func (m *podContainerManagerImpl) IsPodCgroup(cgroupfs string) (bool, types.UID) {
 	// convert the literal cgroupfs form to the driver specific value
 	cgroupName := m.cgroupManager.CgroupName(cgroupfs)
-	qosContainersList := [3]CgroupName{m.qosContainersInfo.BestEffort, m.qosContainersInfo.Burstable, m.qosContainersInfo.Guaranteed}
+	qosContainersList := [6]CgroupName{m.qosContainersInfo.BestEffort, m.qosContainersInfo.Burstable, m.qosContainersInfo.Guaranteed,
+		m.qosContainersInfo.BestEffortSTX, m.qosContainersInfo.BurstableSTX, m.qosContainersInfo.GuaranteedSTX}
 	basePath := ""
 	for _, qosContainerName := range qosContainersList {
 		// a pod cgroup is a direct child of a qos node, so check if its a match
@@ -255,7 +269,8 @@ func (m *podContainerManagerImpl) IsPodCgroup(cgroupfs string) (bool, types.UID)
 func (m *podContainerManagerImpl) GetAllPodsFromCgroups() (map[types.UID]CgroupName, error) {
 	// Map for storing all the found pods on the disk
 	foundPods := make(map[types.UID]CgroupName)
-	qosContainersList := [3]CgroupName{m.qosContainersInfo.BestEffort, m.qosContainersInfo.Burstable, m.qosContainersInfo.Guaranteed}
+	qosContainersList := [6]CgroupName{m.qosContainersInfo.BestEffort, m.qosContainersInfo.Burstable, m.qosContainersInfo.Guaranteed,
+		m.qosContainersInfo.BestEffortSTX, m.qosContainersInfo.BurstableSTX, m.qosContainersInfo.GuaranteedSTX}
 	// Scan through all the subsystem mounts
 	// and through each QoS cgroup directory for each subsystem mount
 	// If a pod cgroup exists in even a single subsystem mount
diff --git a/pkg/kubelet/cm/qos_container_manager_linux.go b/pkg/kubelet/cm/qos_container_manager_linux.go
index aac639c1e65..e7c2e226bf1 100644
--- a/pkg/kubelet/cm/qos_container_manager_linux.go
+++ b/pkg/kubelet/cm/qos_container_manager_linux.go
@@ -25,6 +25,7 @@ import (
 
 	v1 "k8s.io/api/core/v1"
 	"k8s.io/klog/v2"
+	"k8s.io/kubernetes/pkg/kubelet/cm/cpumanager"
 
 	"k8s.io/apimachinery/pkg/util/wait"
 
@@ -82,6 +83,7 @@ func (m *qosContainerManagerImpl) GetQOSContainersInfo() QOSContainersInfo {
 func (m *qosContainerManagerImpl) Start(getNodeAllocatable func() v1.ResourceList, activePods ActivePodsFunc) error {
 	cm := m.cgroupManager
 	rootContainer := m.cgroupRoot
+	stxRootContainer := []string{"k8s-infra-stx", "kubepods"}
 
 	if err := cm.Validate(rootContainer); err != nil {
 		return fmt.Errorf("error validating root container %v : %w", rootContainer, err)
@@ -90,15 +92,17 @@ func (m *qosContainerManagerImpl) Start(getNodeAllocatable func() v1.ResourceLis
 	// Top level for Qos containers are created only for Burstable
 	// and Best Effort classes
 	qosClasses := map[v1.PodQOSClass]CgroupName{
-		v1.PodQOSBurstable:  NewCgroupName(rootContainer, strings.ToLower(string(v1.PodQOSBurstable))),
-		v1.PodQOSBestEffort: NewCgroupName(rootContainer, strings.ToLower(string(v1.PodQOSBestEffort))),
+		v1.PodQOSBurstable:     NewCgroupName(rootContainer, strings.ToLower(string(v1.PodQOSBurstable))),
+		v1.PodQOSBestEffort:    NewCgroupName(rootContainer, strings.ToLower(string(v1.PodQOSBestEffort))),
+		v1.PodQOSBurstableSTX:  NewCgroupName(stxRootContainer, strings.ToLower(string(v1.PodQOSBurstable))),
+		v1.PodQOSBestEffortSTX: NewCgroupName(stxRootContainer, strings.ToLower(string(v1.PodQOSBestEffort))),
 	}
 
 	// Create containers for both qos classes
 	for qosClass, containerName := range qosClasses {
 		resourceParameters := &ResourceConfig{}
 		// the BestEffort QoS class has a statically configured minShares value
-		if qosClass == v1.PodQOSBestEffort {
+		if qosClass == v1.PodQOSBestEffort || qosClass == v1.PodQOSBestEffortSTX {
 			minShares := uint64(MinShares)
 			resourceParameters.CPUShares = &minShares
 		}
@@ -126,9 +130,12 @@ func (m *qosContainerManagerImpl) Start(getNodeAllocatable func() v1.ResourceLis
 	}
 	// Store the top level qos container names
 	m.qosContainersInfo = QOSContainersInfo{
-		Guaranteed: rootContainer,
-		Burstable:  qosClasses[v1.PodQOSBurstable],
-		BestEffort: qosClasses[v1.PodQOSBestEffort],
+		Guaranteed:    rootContainer,
+		Burstable:     qosClasses[v1.PodQOSBurstable],
+		BestEffort:    qosClasses[v1.PodQOSBestEffort],
+		GuaranteedSTX: stxRootContainer,
+		BurstableSTX:  qosClasses[v1.PodQOSBurstableSTX],
+		BestEffortSTX: qosClasses[v1.PodQOSBestEffortSTX],
 	}
 	m.getNodeAllocatable = getNodeAllocatable
 	m.activePods = activePods
@@ -171,6 +178,7 @@ func (m *qosContainerManagerImpl) setHugePagesConfig(configs map[v1.PodQOSClass]
 func (m *qosContainerManagerImpl) setCPUCgroupConfig(configs map[v1.PodQOSClass]*CgroupConfig) error {
 	pods := m.activePods()
 	burstablePodCPURequest := int64(0)
+	burstablePodCPURequestSTX := int64(0)
 	reuseReqs := make(v1.ResourceList, 4)
 	for i := range pods {
 		pod := pods[i]
@@ -185,17 +193,24 @@ func (m *qosContainerManagerImpl) setCPUCgroupConfig(configs map[v1.PodQOSClass]
 			SkipPodLevelResources: !utilfeature.DefaultFeatureGate.Enabled(kubefeatures.PodLevelResources),
 		})
 		if request, found := req[v1.ResourceCPU]; found {
-			burstablePodCPURequest += request.MilliValue()
+			if cpumanager.IsKubeInfra(pod) {
+				burstablePodCPURequestSTX += request.MilliValue()
+			} else {
+				burstablePodCPURequest += request.MilliValue()
+			}
 		}
 	}
 
 	// make sure best effort is always 2 shares
 	bestEffortCPUShares := uint64(MinShares)
 	configs[v1.PodQOSBestEffort].ResourceParameters.CPUShares = &bestEffortCPUShares
+	configs[v1.PodQOSBestEffortSTX].ResourceParameters.CPUShares = &bestEffortCPUShares
 
 	// set burstable shares based on current observe state
 	burstableCPUShares := MilliCPUToShares(burstablePodCPURequest)
+	burstableCPUSharesSTX := MilliCPUToShares(burstablePodCPURequestSTX)
 	configs[v1.PodQOSBurstable].ResourceParameters.CPUShares = &burstableCPUShares
+	configs[v1.PodQOSBurstableSTX].ResourceParameters.CPUShares = &burstableCPUSharesSTX
 	return nil
 }
 
@@ -327,6 +342,18 @@ func (m *qosContainerManagerImpl) UpdateCgroups() error {
 			Name:               m.qosContainersInfo.BestEffort,
 			ResourceParameters: &ResourceConfig{},
 		},
+		v1.PodQOSPlatformGuaranteed: {
+			Name:               m.qosContainersInfo.GuaranteedSTX,
+			ResourceParameters: &ResourceConfig{},
+		},
+		v1.PodQOSBurstableSTX: {
+			Name:               m.qosContainersInfo.BurstableSTX,
+			ResourceParameters: &ResourceConfig{},
+		},
+		v1.PodQOSBestEffortSTX: {
+			Name:               m.qosContainersInfo.BestEffortSTX,
+			ResourceParameters: &ResourceConfig{},
+		},
 	}
 
 	// update the qos level cgroup settings for cpu shares
diff --git a/pkg/kubelet/cm/types.go b/pkg/kubelet/cm/types.go
index e6338d3af81..6f6b5e70130 100644
--- a/pkg/kubelet/cm/types.go
+++ b/pkg/kubelet/cm/types.go
@@ -97,9 +97,12 @@ type CgroupManager interface {
 
 // QOSContainersInfo stores the names of containers per qos
 type QOSContainersInfo struct {
-	Guaranteed CgroupName
-	BestEffort CgroupName
-	Burstable  CgroupName
+	Guaranteed    CgroupName
+	BestEffort    CgroupName
+	Burstable     CgroupName
+	GuaranteedSTX CgroupName
+	BestEffortSTX CgroupName
+	BurstableSTX  CgroupName
 }
 
 // PodContainerManager stores and manages pod level containers
diff --git a/staging/src/k8s.io/api/core/v1/types.go b/staging/src/k8s.io/api/core/v1/types.go
index 08b6d351cc6..e77e4a1adce 100644
--- a/staging/src/k8s.io/api/core/v1/types.go
+++ b/staging/src/k8s.io/api/core/v1/types.go
@@ -4948,6 +4948,12 @@ const (
 	PodQOSBurstable PodQOSClass = "Burstable"
 	// PodQOSBestEffort is the BestEffort qos class.
 	PodQOSBestEffort PodQOSClass = "BestEffort"
+	// PodQOSPlatformGuaranteed is the Guaranteed qos class for STX Platform Pods.
+	PodQOSPlatformGuaranteed PodQOSClass = "GuaranteedSTX"
+	// PodQOSBurstableSTX is the Burstable qos class for STX Platform Pods.
+	PodQOSBurstableSTX PodQOSClass = "BurstableSTX"
+	// PodQOSBestEffortSTX is the BestEffort qos class for STX Platform Pods.
+	PodQOSBestEffortSTX PodQOSClass = "BestEffortSTX"
 )
 
 // PodDNSConfig defines the DNS parameters of a pod in addition to
-- 
2.34.1

